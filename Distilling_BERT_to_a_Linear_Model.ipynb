{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distilling_BERT_to_a_Linear_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev67EeXyugYI",
        "colab_type": "text"
      },
      "source": [
        "Distilling BERT to a Linear Model\n",
        "=================================\n",
        "\n",
        "This is a little extension of the work done in _Distilling Task-Specific Knowledge from BERT into Simple Neural Networks_ by Tang et al. 2019. Hopefully this notebook will serve as an easy-to-follow guide to distillation, which is actually really simple. This is based on work I did for [Polecat](polecat.com).\n",
        "\n",
        "Tang demonstrates that training a lower-complexity student model to predict a teacher model's output logits is more effective than directly training the student model on the dataset. This is a really neat way of improving performance of smaller models (which are much easier to productionize).\n",
        "\n",
        "In the paper Tang uses BERT to train a BiLSTM. One of the suggestions for future work is to explore to what extent even simpler models can benefit from the technique. This notebook does just that - we'll try and use BERT to train a simple linear model implemented in PyTorch.\n",
        "\n",
        "The linear model is the FastText model (Joulin et al. 2016) which normally is an excellent compromise between speed and accuracy. The task is document classification. We wouldn't expect to get near BERT-like accuracy because FastText is a bag-of-words model (it ignores word order, although you can give it n-grams) but it will be interesting to see if we can increase its accuracy at all. \n",
        "\n",
        "Let's begin with our dependencies: PyTorch, the great Huggingface transformers library (for a BERT implementation) and other usual suspects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T8SM4q2SrD8",
        "colab_type": "code",
        "outputId": "dabaa42e-bb8d-4143-9ca5-15f119e29c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "!pip install torch transformers pandas tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.43)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKhjpP3sHljm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from joblib import Memory\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, AdamW, get_linear_schedule_with_warmup, DistilBertForSequenceClassification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOtQVQ9pVGDV",
        "colab_type": "code",
        "outputId": "17589876-91cc-4080-e336-041d28649261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZQZZTqKWxgR",
        "colab_type": "text"
      },
      "source": [
        "Dataset\n",
        "-------\n",
        "\n",
        "We'll use the Amazon review dataset. It is freely available and consists of product reviews with a star rating and the task is simply to predict the star rating.\n",
        "\n",
        "First, some data wrangling. I'm afraid this notebook won't run out-of-the-box, because the data and teacher model are too large to distribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FrGQvQzS7wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA = Path(\"/mnt/gdrive/My Drive/data\")\n",
        "\n",
        "if not DATA.exists():\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/mnt/gdrive\")\n",
        "\n",
        "assert DATA.exists()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-PMOCCl1u7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CACHE = Path(\"/mnt/gdrive/My Drive/cache/distillation\")\n",
        "\n",
        "if not CACHE.exists():\n",
        "    CACHE.mkdir(parents=True)\n",
        "\n",
        "memory = Memory(CACHE, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCNkvexhTHEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "market = \"uk\"\n",
        "\n",
        "reviews = (pd.read_csv(DATA / f\"amazon_reviews_multilingual_{market.upper()}_v1_00.tsv.gz\",\n",
        "                       sep=\"\\t\",\n",
        "                       usecols=[\"review_id\", \"star_rating\", \"review_headline\", \"review_body\"],\n",
        "                       dtype={\"review_id\": \"string\",\n",
        "                              \"star_rating\": \"Int32\",\n",
        "                              \"review_headline\": \"string\",\n",
        "                              \"review_body\": \"string\"})\n",
        "            .dropna())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6o-g2uR5nvN",
        "colab_type": "text"
      },
      "source": [
        "We balance the classes and shuffle the dataset. Ideally we should also remove some low-value reviews, e.g. single-word reviews and reviews in other languages. But there are few enough of these to not make much difference as far as this exploration goes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjO9V73UTK6m",
        "colab_type": "code",
        "outputId": "bea9ad80-909e-485b-a8cd-6c0950f01aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classes = {1, 2, 3, 4, 5}\n",
        "class_examples = [reviews[reviews.star_rating == rating] for rating in classes]\n",
        "\n",
        "MAX_LEN = 50_000\n",
        "\n",
        "min_len = min(MAX_LEN // len(classes), *[len(c) for c in class_examples])\n",
        "\n",
        "balanced_df = pd.concat([c.sample(min_len, random_state=42) for c in class_examples])\n",
        "\n",
        "shuffled_df = balanced_df.sample(len(balanced_df))\n",
        "shuffled_df[\"label\"] = shuffled_df.star_rating.astype(int) - 1\n",
        "\n",
        "len(shuffled_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MalNs-F4TPUb",
        "colab_type": "code",
        "outputId": "e52a04ab-21de-44b0-af21-0bc595717a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "shuffled_df.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1640608</th>\n",
              "      <td>R11NKYEYMR011M</td>\n",
              "      <td>4</td>\n",
              "      <td>Good buy</td>\n",
              "      <td>I bought this sleeve after searching on the in...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297813</th>\n",
              "      <td>R1DS7T8FXIMZO6</td>\n",
              "      <td>3</td>\n",
              "      <td>A Street cat named Bob</td>\n",
              "      <td>Could not put it down. Really easy to read . S...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              review_id  ...  label\n",
              "1640608  R11NKYEYMR011M  ...      3\n",
              "297813   R1DS7T8FXIMZO6  ...      2\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w06iuA7559bF",
        "colab_type": "text"
      },
      "source": [
        "Split the data into a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf0GbuU1TSrf",
        "colab_type": "code",
        "outputId": "5d2ac7af-daa7-45b7-8017-4e3ff2885dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_frac = 0.8\n",
        "split_idx = int(train_frac * len(shuffled_df))\n",
        "\n",
        "train_df = shuffled_df[:split_idx]\n",
        "test_df =shuffled_df[split_idx:]\n",
        "len(train_df), len(test_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55NhgXuN6B_o",
        "colab_type": "text"
      },
      "source": [
        "Tokenize the text and convert it to PyTorch tensors. We also need two masking vectors for each example as input to BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhxUiqpaWPq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "except NameError:\n",
        "    tokenizer = tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmQTwaz-VB_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataframe_to_dataset(df):\n",
        "  max_len = 128\n",
        "  features = tokenizer.batch_encode_plus(df.review_body,\n",
        "                                         max_length=max_len,\n",
        "                                         pad_to_max_length=True,\n",
        "                                         return_attention_masks=True,\n",
        "                                         return_token_type_ids=True,\n",
        "                                         return_tensors=\"pt\")\n",
        "  dataset = TensorDataset(features[\"input_ids\"],\n",
        "                          features[\"attention_mask\"],\n",
        "                          features[\"token_type_ids\"],\n",
        "                          torch.tensor(df.label.astype(\"int\").to_numpy(), dtype=torch.long))\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpfP65ABWq7W",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters\n",
        "---------------\n",
        "\n",
        "These are more-or-less the default hyperparameters for FastText. The embedding dimension is reduced to 50 to speed up processing slightly.\n",
        "\n",
        "Beware the batch size - we're using a batch size of **1** for training. This has a significant impact on the accuracy of the linear model, and it's lightweight enough that we can get away with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os3jL-4jVzii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 5\n",
        "EMBEDDING_DIM = 50\n",
        "LR = 0.5\n",
        "BATCH_SIZE = 32  # for TESTING\n",
        "N_LABELS = 5  # num review ratings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os1muwk5VOn_",
        "colab_type": "code",
        "outputId": "8f5deca3-07ee-4846-aae7-91f26c821b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_loader = DataLoader(dataframe_to_dataset(train_df), batch_size=1, shuffle=False)  # optimal training for the linear model\n",
        "test_loader = DataLoader(dataframe_to_dataset(test_df), batch_size=BATCH_SIZE, shuffle=False)\n",
        "len(train_loader), len(test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 313)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZX0nhVkWtts",
        "colab_type": "text"
      },
      "source": [
        "Teacher\n",
        "-------\n",
        "\n",
        "The teacher is actually DistilBERT, rather than BERT. So we are distilling from a distilled model! Ideally the teacher should be BERT-proper so that results are more comparable. But this is running on Google Colab with limited GPU time, so a compromise is necessary.\n",
        "\n",
        "I trained this DistilBERT model on the same dataset previously. Later on we'll check its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66VmdmCjUjVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    config = config\n",
        "    teacher = teacher\n",
        "except NameError:\n",
        "    config = AutoConfig.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "    config.num_labels = N_LABELS\n",
        "    teacher = DistilBertForSequenceClassification(config)\n",
        "    teacher.load_state_dict(torch.load(DATA / \"distilbert_uk_50000.bin\", map_location=device))\n",
        "_ = teacher.eval()\n",
        "_ = teacher.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RTFFIpuC-t",
        "colab_type": "text"
      },
      "source": [
        "To reduce training time, we cache the teacher's predictions. Using the batches as cache keys require computing a hash each time, which would be computationally expensive, so we take advantage of the lack of shuffling and use the dataset name (\"train\" or \"test\") and the batch number to key the results.\n",
        "\n",
        "If the dataset is shuffled the cache _must_ be cleared.\n",
        "\n",
        "This could be improved by simply labelling the whole dataset before adding it to the data loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lqg92UQrUwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CachingTeacher:\n",
        "\n",
        "    def __init__(self, teacher, cache_path=None):\n",
        "        self.teacher = teacher\n",
        "        self.base_model_prefix = teacher.base_model_prefix\n",
        "        if cache_path is None:\n",
        "            self.cache = {}\n",
        "        else:\n",
        "            self.cache = torch.load(cache_path)\n",
        "\n",
        "    def eval(self):\n",
        "        return self.teacher.eval()\n",
        "\n",
        "    def to(self, device):\n",
        "        return self.teacher.to(device)\n",
        "\n",
        "    def __call__(self, dataset_id, batch_id, **inputs):\n",
        "        cache_id = f\"{dataset_id}_{batch_id}\"\n",
        "        if cache_id in self.cache:\n",
        "            return self.cache[cache_id]\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.teacher(**inputs)\n",
        "            self.cache[cache_id] = outputs\n",
        "            return outputs\n",
        "\n",
        "    def dump(self):\n",
        "        torch.save(self.cache, CACHE / \"distilbert_teacher_cache.bin\")\n",
        "\n",
        "\n",
        "caching_teacher = CachingTeacher(teacher)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di07LSp6WvXx",
        "colab_type": "text"
      },
      "source": [
        "Student\n",
        "-------\n",
        "\n",
        "We define an embeddings bag model as implemented in FastText. For simplicity let's leave out n-grams.\n",
        "\n",
        "We also define `autodidact`, another instance that will be trained on the corpus directly, rather than via supervision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCtKES4bVmeU",
        "colab_type": "code",
        "outputId": "78b156d0-ce51-4518-e9ee-ff67e1f4adff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "class PTModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_vocab, embedding_dim, n_labels, padding_idx):\n",
        "        super(PTModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(n_vocab, embedding_dim, padding_idx=padding_idx)\n",
        "        self.output = nn.Linear(embedding_dim, n_labels)\n",
        "        with torch.no_grad():\n",
        "            # FastText initializes embeddings with uniform distribution vs normal in PyTorch\n",
        "            self.embeddings.weight.uniform_(to=1.0 / embedding_dim)\n",
        "            self.embeddings.weight[padding_idx] = 0  # but FT doesn't have a padding token\n",
        "            # FastText initializes output with zeros vs some random dist in PyTorch\n",
        "            self.output.weight.zero_()\n",
        "\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "        \"\"\"Only input ids are required - kwargs are for API compat with BERT.\"\"\"\n",
        "        X = self.embeddings(input_ids)\n",
        "        X = X.mean(dim=1)\n",
        "        X = self.output(X)\n",
        "        return X\n",
        "    \n",
        "padding_idx = tokenizer.vocab[\"[PAD]\"]\n",
        "n_vocab = len(tokenizer.vocab)\n",
        "student = PTModel(n_vocab, EMBEDDING_DIM, N_LABELS, padding_idx)\n",
        "student.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PTModel(\n",
              "  (embeddings): Embedding(119547, 50, padding_idx=0)\n",
              "  (output): Linear(in_features=50, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94eRJCO_oNnj",
        "colab_type": "code",
        "outputId": "19cd92b7-9e24-4500-cb63-92804fe54a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "autodidact = PTModel(n_vocab, EMBEDDING_DIM, N_LABELS, padding_idx)\n",
        "autodidact.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PTModel(\n",
              "  (embeddings): Embedding(119547, 50, padding_idx=0)\n",
              "  (output): Linear(in_features=50, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otPP3--DW_58",
        "colab_type": "text"
      },
      "source": [
        "Training\n",
        "--------\n",
        "\n",
        "This function trains the model for one epoch. If no teacher is provided it uses cross entropy loss (i.e. softmax then NLL) and compares the model predictions to the target label.\n",
        "\n",
        "If a teacher is provided then model predictions are compared to the teacher's predictions and MSE loss is used.\n",
        "\n",
        "In the paper Tang defines a cost function that is a balance between the two (i.e. $L = \\alpha L_{CE} + (1 - \\alpha L_{MSE})$ but in practice observed that the best value for $\\alpha$ was zero.\n",
        "\n",
        "The accuracy on the training set is also output for visibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqpDH4XtXAyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(train_iter, model, optim, epoch_num, teacher=None):\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    \n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    \n",
        "    if teacher is not None:\n",
        "        teacher.to(device)\n",
        "        teacher.eval()\n",
        "        cost = nn.MSELoss()\n",
        "    else:\n",
        "        cost = nn.CrossEntropyLoss()\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm(train_iter, total=len(train_iter), desc=f\"Batch progress for epoch {epoch_num}\")):\n",
        "        \n",
        "        batch = tuple([t.to(device) for t in batch])\n",
        "        inputs = {\"input_ids\": batch[0],\n",
        "                  \"attention_mask\": batch[1]}\n",
        "        labels = batch[3]\n",
        "\n",
        "        optim.zero_grad()\n",
        "        output = model(**inputs)\n",
        "\n",
        "        if teacher is not None:\n",
        "            if teacher.base_model_prefix == \"bert\":\n",
        "                inputs[\"token_type_ids\"]: batch[2]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                target = teacher(\"train\", batch_idx, **inputs)[0]  # BERT returns a tuple\n",
        "        else:\n",
        "            target = labels\n",
        "\n",
        "        batch_loss = cost(output, target)\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "        batch_acc = (output.argmax(1) == labels).sum().item()\n",
        "        train_acc += batch_acc\n",
        "\n",
        "        #print(f\"{i:03}: {batch_loss.item() / len(labels):.03}\\t\\t{(batch_acc / len(labels)):.03}\")\n",
        "        \n",
        "        batch_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.embeddings.parameters(), 1)  # FT only normalizes the embeddings grad\n",
        "        optim.step()\n",
        "\n",
        "    return train_loss / len(train_iter.dataset), train_acc / len(train_iter.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqK2kyVK_ZEO",
        "colab_type": "text"
      },
      "source": [
        "The validation function is similar but in this case there is no option to compare to the teacher's predictions, because that's not the ultimate point of the exercise - at the end of it all we just want a better small model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYvXJauRnc7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(test_iter, model):\n",
        "    test_acc = 0 \n",
        "    test_loss = 0\n",
        "    \n",
        "    cost = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(test_iter, desc=\"Validating\"):\n",
        "        \n",
        "        batch = tuple([t.to(device) for t in batch])\n",
        "        inputs = {\"input_ids\": batch[0],\n",
        "                  \"attention_mask\": batch[1],\n",
        "                  \"token_type_ids\": batch[2]}\n",
        "        labels = batch[3]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(**inputs)\n",
        "            \n",
        "            batch_loss = cost(output, labels)\n",
        "            test_loss += batch_loss.item()\n",
        "                    \n",
        "            batch_acc = (output.argmax(1) == labels).sum().item() \n",
        "            test_acc += batch_acc\n",
        "            \n",
        "    return test_loss / len(test_iter.dataset), test_acc / len(test_iter.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxboj94idLIP",
        "colab_type": "text"
      },
      "source": [
        "Sanity check - we expect 20% accuracy in each case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t84_mQhGdJ-B",
        "colab_type": "code",
        "outputId": "33042e9a-1299-4ab9-b180-c7b322c29fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "validate(test_loader, student)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1113.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.050443163526058196, 0.1954)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUtRBjRpn13o",
        "colab_type": "code",
        "outputId": "999c8df3-8f44-4f65-e73d-b513ccc6b0de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "validate(test_loader, autodidact)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1191.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.05050554784536362, 0.2024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzgzIprndMye",
        "colab_type": "text"
      },
      "source": [
        "Training\n",
        "--------\n",
        "\n",
        "We use SGD and a linearly decreasing learning rate because this is empirically best for the linear model (see Joulin et al.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf8Jd5TWY78z",
        "colab_type": "code",
        "outputId": "0f8e36eb-820a-4ec5-ddae-0476447c5f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "optim = torch.optim.SGD(student.parameters(), lr=LR)\n",
        "sched = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=0.5)\n",
        "\n",
        "training_results = {\"train_loss\": [],\n",
        "                    \"train_acc\": [],\n",
        "                    \"test_loss\": [],\n",
        "                    \"test_acc\": []}\n",
        "\n",
        "student.to(device)\n",
        "teacher.to(device)\n",
        "\n",
        "try:\n",
        "    for i in range(N_EPOCHS):\n",
        "        train_loss, train_acc = train_epoch(train_loader, student, optim, epoch_num=i, teacher=caching_teacher)\n",
        "        sched.step()\n",
        "        test_loss, test_acc = validate(test_loader, student)\n",
        "        training_results[\"train_loss\"].append(train_loss)\n",
        "        training_results[\"train_acc\"].append(train_acc)\n",
        "        training_results[\"test_loss\"].append(test_loss)\n",
        "        training_results[\"test_acc\"].append(test_acc)\n",
        "        print(f\"{i:02}: {train_loss:.03} {train_acc:.03} {test_loss:.03} {test_acc:.03}\")\n",
        "        torch.save({'state_dict': student.state_dict()}, DATA / f\"student_{market}_{len(shuffled_df)}.bin\")\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "train_df = pd.DataFrame(training_results)\n",
        "train_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch progress for epoch 0: 100%|██████████| 40000/40000 [04:03<00:00, 164.09it/s]\n",
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1261.51it/s]\n",
            "Batch progress for epoch 1:   0%|          | 74/40000 [00:00<00:54, 734.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00: 2.1 0.376 0.0412 0.425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch progress for epoch 1: 100%|██████████| 40000/40000 [00:53<00:00, 753.01it/s]\n",
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1218.56it/s]\n",
            "Batch progress for epoch 2:   0%|          | 77/40000 [00:00<00:52, 767.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01: 1.53 0.455 0.04 0.449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch progress for epoch 2: 100%|██████████| 40000/40000 [00:53<00:00, 753.31it/s]\n",
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1186.13it/s]\n",
            "Batch progress for epoch 3:   0%|          | 74/40000 [00:00<00:54, 738.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02: 1.41 0.472 0.0394 0.46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch progress for epoch 3: 100%|██████████| 40000/40000 [00:53<00:00, 753.73it/s]\n",
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1223.24it/s]\n",
            "Batch progress for epoch 4:   0%|          | 68/40000 [00:00<00:59, 675.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "03: 1.35 0.479 0.0391 0.468\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch progress for epoch 4: 100%|██████████| 40000/40000 [00:52<00:00, 754.75it/s]\n",
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1239.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "04: 1.32 0.482 0.039 0.475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.103212</td>\n",
              "      <td>0.375700</td>\n",
              "      <td>0.041180</td>\n",
              "      <td>0.4255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.530487</td>\n",
              "      <td>0.455025</td>\n",
              "      <td>0.039959</td>\n",
              "      <td>0.4492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.409630</td>\n",
              "      <td>0.472300</td>\n",
              "      <td>0.039385</td>\n",
              "      <td>0.4604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.352989</td>\n",
              "      <td>0.479475</td>\n",
              "      <td>0.039142</td>\n",
              "      <td>0.4676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.323567</td>\n",
              "      <td>0.482175</td>\n",
              "      <td>0.038977</td>\n",
              "      <td>0.4754</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   train_loss  train_acc  test_loss  test_acc\n",
              "0    2.103212   0.375700   0.041180    0.4255\n",
              "1    1.530487   0.455025   0.039959    0.4492\n",
              "2    1.409630   0.472300   0.039385    0.4604\n",
              "3    1.352989   0.479475   0.039142    0.4676\n",
              "4    1.323567   0.482175   0.038977    0.4754"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcr_xZg6Ae98",
        "colab_type": "text"
      },
      "source": [
        "A second training loop for the \"autodidact\", the model that is trained without a teacher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clbIp15Km536",
        "colab_type": "code",
        "outputId": "253db2d0-077c-443c-e966-e6e0f6eb9e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "optim_autodidact = torch.optim.SGD(autodidact.parameters(), lr=LR)\n",
        "sched_autodidact = torch.optim.lr_scheduler.StepLR(optim_autodidact, step_size=1, gamma=0.5)\n",
        "\n",
        "training_results = {\"train_loss\": [],\n",
        "                    \"train_acc\": [],\n",
        "                    \"test_loss\": [],\n",
        "                    \"test_acc\": []}\n",
        "\n",
        "autodidact.to(device)\n",
        "\n",
        "try:\n",
        "    for i in range(N_EPOCHS):\n",
        "        train_loss, train_acc = train_epoch(train_loader, autodidact, optim_autodidact, epoch_num=i)\n",
        "        sched_autodidact.step()\n",
        "        test_loss, test_acc = validate(test_loader, autodidact)\n",
        "        training_results[\"train_loss\"].append(train_loss)\n",
        "        training_results[\"train_acc\"].append(train_acc)\n",
        "        training_results[\"test_loss\"].append(test_loss)\n",
        "        training_results[\"test_acc\"].append(test_acc)\n",
        "        print(f\"{i:02}: {train_loss:.03} {train_acc:.03} {test_loss:.03} {test_acc:.03}\")\n",
        "        torch.save({'state_dict': student.state_dict()}, DATA / f\"autodidact_{market}_{len(shuffled_df)}.bin\")\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "train_df_autodidact = pd.DataFrame(training_results)\n",
        "train_df_autodidact"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch progress for epoch 0: 100%|██████████| 40000/40000 [00:54<00:00, 737.42it/s]\n",
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1185.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00: 1.52 0.33 0.0441 0.389\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch progress for epoch 1: 100%|██████████| 40000/40000 [00:54<00:00, 739.09it/s]\n",
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1232.12it/s]\n",
            "Batch progress for epoch 2:   0%|          | 75/40000 [00:00<00:53, 748.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01: 1.31 0.437 0.0409 0.433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch progress for epoch 2: 100%|██████████| 40000/40000 [00:54<00:00, 738.93it/s]\n",
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1197.24it/s]\n",
            "Batch progress for epoch 3:   0%|          | 75/40000 [00:00<00:53, 746.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02: 1.24 0.469 0.0406 0.444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch progress for epoch 3: 100%|██████████| 40000/40000 [00:53<00:00, 743.98it/s]\n",
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1169.77it/s]\n",
            "Batch progress for epoch 4:   0%|          | 72/40000 [00:00<00:55, 716.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "03: 1.21 0.489 0.0405 0.446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch progress for epoch 4: 100%|██████████| 40000/40000 [00:53<00:00, 742.16it/s]\n",
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1224.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "04: 1.2 0.498 0.0401 0.451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.521084</td>\n",
              "      <td>0.329625</td>\n",
              "      <td>0.044074</td>\n",
              "      <td>0.3888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.309855</td>\n",
              "      <td>0.436775</td>\n",
              "      <td>0.040916</td>\n",
              "      <td>0.4331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.243494</td>\n",
              "      <td>0.469450</td>\n",
              "      <td>0.040609</td>\n",
              "      <td>0.4442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.211850</td>\n",
              "      <td>0.489200</td>\n",
              "      <td>0.040522</td>\n",
              "      <td>0.4460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.196159</td>\n",
              "      <td>0.498400</td>\n",
              "      <td>0.040130</td>\n",
              "      <td>0.4514</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   train_loss  train_acc  test_loss  test_acc\n",
              "0    1.521084   0.329625   0.044074    0.3888\n",
              "1    1.309855   0.436775   0.040916    0.4331\n",
              "2    1.243494   0.469450   0.040609    0.4442\n",
              "3    1.211850   0.489200   0.040522    0.4460\n",
              "4    1.196159   0.498400   0.040130    0.4514"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1DLajiEAnFU",
        "colab_type": "text"
      },
      "source": [
        "It's interesting to see that the student converged noticeably faster than the directly-trained model (look at the `train_acc` and `test_acc` columns). Note that you cannot directly compare the training loss, remember these are from different loss functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ng30VxDa-rR",
        "colab_type": "text"
      },
      "source": [
        "Results\n",
        "-------\n",
        "\n",
        "This is a little bit redundant because it was already output in the training loop, but it's helpful to look at it in isolation.\n",
        "\n",
        "Let's evaluate student on test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EcVxigKZ8q1",
        "colab_type": "code",
        "outputId": "7a662e10-332c-4246-9c1c-1c3e3524705e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "validate(test_loader, student)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1189.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.038976528859138486, 0.4754)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzoL1P6lo5cA",
        "colab_type": "text"
      },
      "source": [
        "And then compare with the accuracy of the model that was directly trained (no distillation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_SZcnm5lz8M",
        "colab_type": "code",
        "outputId": "5a8d04b3-4cad-4573-d974-c28535f3ee64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "validate(test_loader, autodidact)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validating: 100%|██████████| 313/313 [00:00<00:00, 1194.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.04013044349551201, 0.4514)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVpO4MAolj3T",
        "colab_type": "text"
      },
      "source": [
        "The student does score slightly higher accuracy but let's not assume this is statistically significant. (If we were serious, we would whip out a package like `statsmodels` and check.)\n",
        "\n",
        "For comparison, this is the teacher's result on the test set (it was also trained for 5 epochs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP-97NQElY9s",
        "colab_type": "code",
        "outputId": "8051a3b0-39f4-4086-ccf8-961e82149e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "teacher_test_acc5 = []\n",
        "\n",
        "for batch_num, batch in enumerate(tqdm(test_loader)):\n",
        "    batch = tuple([t.to(device) for t in batch])\n",
        "    inputs = {\"input_ids\": batch[0],\n",
        "              \"attention_mask\": batch[1]}\n",
        "    if caching_teacher.base_model_prefix == \"bert\":\n",
        "        inputs[\"token_type_ids\"]: batch[2]\n",
        "    labels = batch[3]\n",
        "    with torch.no_grad():\n",
        "        logits = caching_teacher(\"test\", batch_num, **inputs)[0]\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        preds_5class = probs.argmax(dim=1)\n",
        "        acc_5class = (preds_5class == labels).sum().item() / len(batch[0])\n",
        "        teacher_test_acc5.append(acc_5class)\n",
        "        \n",
        "np.mean(teacher_test_acc5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:18<00:00, 16.64it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.62310303514377"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBkrb-UCj5c",
        "colab_type": "text"
      },
      "source": [
        "(Really brief) Discussion\n",
        "-------------------------\n",
        "\n",
        "So neither model really got close to the teacher's accuracy, and the student was probably not significantly more accurate. Perhaps this is simply as accurate as we can expect that architecture to be on this task (remember it is a BOW model with no n-grams).\n",
        "\n",
        "But it's not a total bust. It's very interesting to see that the student did converge faster. This supports Tang's suggestion that the information about prediction uncertainty is valuable, and that this even outweighs the error from the teacher's inaccurate predictions.\n",
        "\n",
        "We might get better results if we implement the data augmentation that Tang suggests. We could also probably do better with a more complex student - you can see that in this [NLP Town blog post](https://www.nlp.town/blog/distilling-bert/), which inspired me to try this. NLP Town trained a CNN, which you would expect to perform better because eit is more sophisticated and is able to account for the immediate context around a word (useful for e.g. negation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KApe3Xvlozqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}